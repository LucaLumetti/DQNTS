\documentclass{article}
\input{structure.tex}
\usepackage[ruled,vlined]{algorithm2e}

\title{Automated Decision Making: Final Project}
\author{Luca Lumetti\\ \texttt{244577@studenti.unimore.it}}

\begin{document}
\maketitle

\section{Introduction}
In this project I've tried to face the Max-Mean Dispersion Problem using Tabu
Search guided by deep reinforcement learning during the dispersion fase.\\
Given a complete graph G(V, E) where each edge has associated a distance or
affinity $d_{ij}$, the Max Mean Dispersion Problem is the search of a subset of
vertex $M \subset V, |M| >= 2$ which maximize the dispersion, calcuated as
follow:
$$
MeanDispersion(M) = \frac{\sum_{i<j; i,j \in M}d_{ij}}{|M|}
$$
% Qua forse posso scrivere altro, parlare del modello MILP, che puÃ² essere usato
% sotto le 100 sol e ad altri metodi/heuristiche usate per attaccare il problema
% In in 2013, Martin and Sandoya proposed the Greedy Random Adaptive Search integrated with the Path
% Relinking Method (GRASP-PR) [18]. It uses a randomized greedy mechanism to
% maintain first building elite solutions (ES) and a variable neighborhood descent
% procedure for improvement.
\section{RLTS}
In 2020, Nijimbere et al. proposed an approach based on the
combination of reinforcement learning and tabu search, named RLTS. The main idea is to use
Q-Learning to build an high-quality initial solution, then the
initial solution is improved with a one-flip tabu search algorithm.\\

\section{DQNTS}
The main idea is to let a network to learn an heuristich to build the initial
solution using Deep Q-Learning, which can generalize to graphs of any size, then use one-flip tabu
search to improve that solution as in \cite{nijimbere2020tabu}. In this case the
Q-Learning algorithm should generate better solutions than RLTS at the initial interations.

\subsection{Network Architecture}
The network architecture is based on \cite{nijimbere2020tabu}, the
hyperparameters setting can be seen at www.github.com. The state2tens embedding
is done with 4 features extracted from each node, which are:
\begin{itemize}
  \item{1 if the node in in the solution, 0 otherwise}
  \item{the sum of all edges connected to the node}
  \item{the sum of all edges connected to the node and the solution nodes}
  \item{the sum of all edges connected to the node and the nodes not in the solution}
\end{itemize}

\subsection{Network training}
To train the network, first we construct a feasible solution using an
$\epsilon$-greedy strategy, stopping when no positive rewards are predicted by
the network.  Then the solution is given to the one-flip tabu search which
return the best solution.  For every node in the initial solution which remains
in the final solution after tabu search, a reward of +1 is given, otherwise the
reward is -1 The network has been trained over 10 different instances (MDPIx\_35
and MDPIIx\_35, $1 <= x <= 10$) for 5001 episodes.

\subsection{Tabu Search}
The tabu search implementation is the same as in \cite{nijimbere2020tabu}, with the only
difference in the parameter $\alpha = 100$ instead of $\alpha = 50000$. My
implementation couldn't finish even a single iteration in the time limit imposed
with the parameter $\alpha$ proposed in the paper. This made me think that my
implementation is way slower, but still i left the same
time contraints as the results were good enough.\\

\subsection{General Algorithm}
The network architecture is used during the contruction of the inital solution:
for all the nodes, the network estimate the reward, then all the values get
interpolated in the range $[-1,+1]$ and all the nodes $>= 0$ are named as "good
nodes". Among these "good nodes", a random amount is taken to construct the
initial solution. Then this solution is processed with one-flip tabu search
until no best solutions are found for $\alpha = 100$ iterations in a row.
Now a new initial solution is generated and the process is repeated again, until
the time limit is not violated. Finally the best solution found is returned.

% \section{Results}
% \input{results.tex}

\bibliography{main}
\bibliographystyle{plain}
\end{document}
